1.F1 score -> является средним гармоническим между точности и полноты, что позволяет точнее оценить модель.

2.Независимые выборки - сравниваются две разные группы, например мужчины и женщины, молодые и пожилые и т.д. Зависимые выборки как правило возникают, когда речь идет об одной группе испытуемых до и после эксперементального воздействия. 

3.Методы непараметрической статистики оказываются более консервативными по сравнению с методами параметрической статистики.
Их использование грозит в большей мере ошибкой когда мы не может обнаружить отличия двух выборок, когда такие отличия на самом деле имеют место. 
Такие методы оказываются менее мощными по сравнению с параметрическими методами. 
Поэтому использование параметрической статистики в анализе экспериментальных данных, отличающихся от простого ранжирования, как правило, является предпочтительным.

4.Если у классов разное количество выборок, правильнее может быть использование макроусреднения

5. XGBoost - нет работы(хуже) с категориальными параметрами\признаками, у остальных есть

6. Если я праввильно понял вопрос, то мы просто вводим точные условия на выборку, что сразу уберает часть значений из выборки. 
Так же мы можем "стричь" деревья, для лучшего понимания модели и простоты ее расчета в будущем. Разбивка на несколько поддеревьев выборку для категориальных признаков
7. Высчитывается среднее от нескольких деревьев таким образом уменьшая ошибку конечной 
